### Hyperparameter Optimization: Cost-Efficient Stock Forecasting with Enhanced LightGBM üìäüí°

In this section, we delve into the hyperparameter optimization process, a critical step in enhancing the performance of the LightGBM model for stock prediction. Let's break down the concepts, terminologies, and methodologies with detailed explanations and real-world examples.

#### What are Hyperparameters? ü§î
Hyperparameters are settings or configurations that control the learning process of a machine learning model. Unlike model parameters (like weights in a neural network), hyperparameters are not learned from the data during training. They need to be set before the training process begins.

- **Example**: Think of hyperparameters as the settings on your oven (temperature, cooking time) when baking a cake. They need to be set before you start baking, whereas the actual baking process (model training) adjusts the ingredients (parameters) inside the oven.

#### Optuna Framework: Automated Hyperparameter Optimization üõ†Ô∏è

The research utilizes the **Optuna framework**, an open-source tool designed for optimizing hyperparameters efficiently. Optuna automates the search for the best hyperparameters by using different sampling methods like GridSampler, RandomSampler, and TPESampler.

- **Example**: Imagine you are trying to find the perfect settings for your oven. Instead of manually trying each combination, Optuna acts like a smart assistant that tries various settings for you and finds the best combination that bakes the perfect cake. üéÇ

#### Cross-Validation with Time Series Data ‚è≥

**Cross-validation** is a technique used to evaluate the performance of a model by partitioning the data into training and testing sets multiple times. In time series data, an 'n-year sliding window' approach is often used, but selecting the right 'n' can be challenging.

- **Example**: If you want to test the durability of a product, you would use it in different conditions over time to ensure it performs well consistently. Similarly, cross-validation tests the model's performance over different time periods to ensure reliability.

For this study, three training sets were considered:
1. 2010-2012
2. 2010-2015
3. 2010-2018

The test sets were the subsequent years: 2013, 2016, and 2019, respectively.

![Time Series Cross-Validation](https://via.placeholder.com/150)

#### Hyperparameters Adjusted with Optuna üìà

Here are some key hyperparameters optimized in the study, along with their implications:

1. **lambda_l1**: 8.52 - L1 regularization to prevent overfitting.
2. **lambda_l2**: 1.23e-05 - L2 regularization to prevent overfitting.
3. **num_leaves**: 143 - Number of leaves in one tree, controlling the complexity.
4. **feature_fraction**: 0.9 - Fraction of features used in each iteration.
5. **bagging_fraction**: 1.0 - Fraction of data used for bagging.
6. **bagging_freq**: 0 - Frequency of bagging.
7. **min_child_samples**: 50 - Minimum number of samples per leaf.

#### Terminologies Explained:

1. **Regularization (L1 and L2)**:
   - Techniques to prevent overfitting by adding a penalty for larger coefficients in the model. üõ°Ô∏è
   - **L1 Regularization (Lasso)**: Adds absolute values of coefficients. It can shrink some coefficients to zero, effectively performing feature selection. 
   - **L2 Regularization (Ridge)**: Adds squared values of coefficients, which helps to keep all features but reduces their impact.

2. **Num Leaves**:
   - Controls the complexity of the tree. More leaves can capture more information but may lead to overfitting. üçÉ

3. **Feature Fraction**:
   - The fraction of features used to build each tree. It helps in reducing overfitting by introducing randomness. üìä

4. **Bagging Fraction and Frequency**:
   - **Bagging Fraction**: The proportion of data used for each iteration of bagging. üß©
   - **Bagging Frequency**: How often bagging is performed. Setting it to zero means bagging is done in every iteration. üîÑ

5. **Min Child Samples**:
   - The minimum number of samples a leaf must have. It prevents the model from creating leaves with few samples, which can cause overfitting. üå±

---

### Table of Contents
1. [Introduction](https://github.com/aditya-saxena-7/Cost-Efficient-Stock-Forecasting-with-Enhanced-LightGBM/blob/master/Introduction.md)
2. [Model and Methodology](https://github.com/aditya-saxena-7/Cost-Efficient-Stock-Forecasting-with-Enhanced-LightGBM/blob/master/Model%20and%20Methodology.md)
3. [Data Descriptions and Feature Engineering](https://github.com/aditya-saxena-7/Cost-Efficient-Stock-Forecasting-with-Enhanced-LightGBM/blob/master/Data%20Descriptions%20and%20Feature%20Engineering.md)
4. [Hyperparameter Optimization](https://github.com/aditya-saxena-7/Cost-Efficient-Stock-Forecasting-with-Enhanced-LightGBM/blob/master/Hyperparameter%20Optimization.md)
5. [Cost Awareness](https://github.com/aditya-saxena-7/Cost-Efficient-Stock-Forecasting-with-Enhanced-LightGBM/blob/master/Cost%20Awareness.md)
6. [Performance and Measurement](https://github.com/aditya-saxena-7/Cost-Efficient-Stock-Forecasting-with-Enhanced-LightGBM/blob/master/Performance%20and%20Measurement.md)
7. [Conclusion](https://github.com/aditya-saxena-7/Cost-Efficient-Stock-Forecasting-with-Enhanced-LightGBM/blob/master/Conclusion.md)
